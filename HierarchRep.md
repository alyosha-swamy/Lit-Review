In this blog post, we will be delving into a groundbreaking research paper titled "Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-the-Wild Images," which has been accepted at CVPR in 2023. The paper addresses a fundamental limitation of 3D morphable models that hinders their ability to capture fine-grained details, such as wrinkles and dimples, leading to inaccuracies in face reconstruction methods.

The primary motivation behind this research stems from the shortcomings of existing 3D EMMA models, which fail to capture high-frequency facial expressions effectively. As a result, researchers have been actively exploring novel approaches to incorporate more details, thus enhancing the accuracy and precision of capturing various nuances in facial features.

The proposed solution in this paper introduces a novel hierarchical reconstruction network tailored for single-view face reconstruction tasks. This innovative strategy enables the system to overcome the aforementioned limitations and produce remarkably high-quality and accurate face reconstructions, surpassing current leading methods in the field.

The concept of the hierarchical representation network revolves around breaking down the facial geometry into distinct levels, each layer adding unique additional features to the previous one. The process starts with extracting identity coefficients and identity and expression coefficients using the 3D morphable model. Subsequently, a deformation map, termed "medium frequency detailed expressions," is added to provide more information about the jawline and local facial shape corresponding to the 3D model. Finally, the "high frequency facial details" stage aims to capture intricate facial elements like wrinkles and lines around the forehead and other subtle expressions crucial for faithfully reconstructing diverse faces.

To further enhance the results, the researchers introduce an innovative technique known as the "D retouching module," designed to effectively separate facial geometry and appearance. While an albedo map can provide color information for each input image, it may not sufficiently capture skin texture, such as moles, scars, or freckles. The retouching module rectifies this issue by adding more information to the albedo map, leading to more faithful facial reconstructions.

Now, let's briefly explore the related work in the domain of single-view face reconstruction. Recovering a 3D face from a single image is a challenging and ill-posed problem, but thanks to the development of 3D morphable models, significant progress has been made in this area. The focus of this paper lies within the realm of single-view face reconstruction, where researchers aim to utilize a single input image to reconstruct a comprehensive 3D representation of the face.

The original research that laid the foundation for the 3D morphology model aimed to investigate how the visual system processes diverse images of a single object and how these objects are represented to accomplish vision-related tasks. In the late 1990s, a critical assumption was made to develop the 3D morphology model, which involved incorporating significant prior knowledge about a specific class or region to tackle complex problems effectively.

The 3D Morphable Model (3DMM) was designed to encapsulate and capture this prior knowledge, which is automatically learned from various examples provided within a specific domain. To illustrate the process, a 3D database containing hundreds of scanned faces was used. By analyzing these 3D faces, researchers identified common features shared among human faces. Principal Component Analysis (PCA) was then employed to capture the essential aspects of a human face, such as the location of the eyes and the typical shape of the nose, within different facial geometries.

The outcome of this analysis was the creation of a statistical model called the Morphable Face Model (MFM). The MFM encapsulates basic information about human faces but does not contain specific facial details. However, it exhibits complex facial geometry, enabling it to represent various human faces based on input coefficients.

In modern implementations, researchers use face analyzers, like the classic ResNet-15, to extract features from 2D input images. These features are then fed into the MFM, which accepts specific input coefficients corresponding to factors like identity, expressions, and illumination. With these inputs, the MFM reconstructs the 2D input into a 3D output, effectively representing the human face in a detailed and accurate manner.

The 3D morphology model, derived from a collection of scanned faces and reduced using PCA, allows for controlled manipulation by providing specific coefficients. However, its limitation lies in the low-dimensional linear space it inherently occupies due to PCA's extraction of basic human facial features. As a result, the model lacks fine details, such as wrinkles and dimples, that are essential for realistic representations of human faces.

To overcome this limitation and achieve highly accurate and faithful face reconstructions, researchers proposed a novel model architecture. Initially, they employed the 3D model to predict the coarse mesh or an albedo map. The input portrait was fed to a face analyzer, which utilized ResNet-15 to extract coefficients for the 3D morphology model. These coefficients were then used as input for the modeler, which produced M0, representing the coarse mesh, along with the albedo map and the lightning M, forming the base for face reconstruction.

To handle the complex facial details in a coarse-to-fine manner, additional information was incorporated into the 3D prior using a hierarchical modeling strategy represented by the green and purple areas in the architecture diagram. The purpose of these regions was to capture fine-grained facial details and enhance the 3DMM's fidelity. To achieve this, adversarial and semi-supervised learning techniques were employed.

Specifically, to reconstruct the face faithfully, the model required additional information, including the deformation map and the displacement map. To obtain these, a guided network called PIX2PIX, a classical conditional generative adversarial network (CGAN), was utilized. PIX2PIX generated the deformation map and the displacement map, crucial for the accurate and detailed representation of the reconstructed face.

By combining the 3DMM with the hierarchical reconstruction network, the proposed method surpassed existing approaches in accurately reconstructing and capturing fine facial details, resulting in more lifelike and realistic representations of human faces. The integration of additional information through adversarial and semi-supervised learning techniques further contributed to the model's high accuracy and faithfulness in face reconstruction.

The utilization of adversarial and semi-supervised learning techniques in the proposed hierarchical representation network enables the model to capture fine details and improve facial reconstruction accuracy significantly. Additionally, the introduction of the de-retouching module plays a crucial role in decoupling facial geometry and appearance, effectively addressing issues related to skin texture and illumination ambiguity. This module enhances the albedo map (A0) by capturing detailed information such as freckles, moles, and other skin-related features.

The hierarchical modeling approach adopted in the research involves decomposing the facial geometry into three components to capture different levels of information. Firstly, the low-frequency part provides a coarse shape that is roughly aligned to the input image. This step utilizes the classical 3D model by using identity and expression coefficients obtained from the face analyzer to reconstruct the low-frequency part.

Next, the mid-frequency details are incorporated to describe contour and local shape information relative to the low-frequency part. This is achieved using the deformation map, which provides information on how each vertex should move in a specific direction to enhance the 3D morphable model's output with additional details, particularly addressing areas like the jawline that the model may not handle effectively.

Finally, the high-frequency detail stage is introduced to capture individual-specific details like wrinkles and micro bumps, which are unique to each person's face. The high-frequency details are handled using the displacement map, which precisely denotes the geometry deformation along the normal directions. This map allows for rendering very tiny but crucial details that the base model's vertex density may not capture adequately.

The general flow of the proposed model involves employing a base face model to reconstruct the low-frequency part based on the input image's coefficient. The three-channel deformation map is then applied to enable flexible geometry manipulations, and for the high-frequency details, a displacement map is used to render tiny but essential facial features.

In the pre-training phase, the researchers use a regressor network, likely based on ResNet-15, to analyze portrait images and extract 3DMM coefficients, which are then fed into the modeler. The modeler produces the coarse aligned mesh (M0) and albedo map (A0), forming the basis for face reconstruction. Additionally, the data generation process involves mapping the input image and M0 to obtain the position map (P) and texture map (T), which are used as inputs for the training phase.

In the training reconstruction phase, a pixel-to-pixel network (CGAN) is employed to sequentially synthesize the deformation map and the displacement map. This network utilizes detailed information from texture maps (T) and a pixel-wise learning strategy to generate accurate detail maps. The deformation map, representing medium-frequency details, and the displacement map, containing high-frequency details like wrinkles, are used in a coarse-to-fine manner to generate face meshes M1 and M2.

The loss function used in the training comprises three components: reconstruction loss, detail loss, and contour-aware loss. The reconstruction loss involves classical techniques such as photometric loss, perception-level loss, and landmark loss, calculated between the rendered faces and the input faces. These losses aid in the model's generalization on diverse datasets.

To ensure disentanglement of mid-frequency and high-frequency details, two-dimensional reconstruction loss is applied to images rendered from M1 and M2. This process allows the model to calculate the loss from each part separately, ensuring that the high-frequency details are learned effectively, and resulting in improved reconstruction performance.

The model's training also incorporates adversarial loss functions for both the deformation map and the displacement map. These adversarial losses help the model capture fine details and achieve more accurate facial reconstructions.

The two-stage reconstruction approach in the proposed hierarchical model enables the model to differentiate between mid-frequency and high-frequency details, leading to more accurate and realistic facial reconstructions. During each stage, the model feeds the coarse mesh M1 and M2 and calculates the corresponding loss, effectively learning to distinguish between different levels of facial details.

The detail loss, represented by the total variation loss (LTV) and L1 regularization, encourages the smoothness of the deformation map and limits the scale of the displacement map, further improving the quality of facial reconstruction.

The contour-aware loss is a novel contribution in this paper and aims to achieve accurate modeling of the face contour. By projecting the vertex of M1 and M2 to the input image space and predicting the face mesh M_face using the pre-trained face matching network, the researchers identify vertex points that are outside the projected face mask. These points are then pulled back into the face mesh to ensure a smooth and accurate jawline and improve the shape around the edge of the vertex.

Next, the researchers focus on the 3D prior detail of the face model. While facial details can be roughly learned from a single image using reconstruction loss, this approach may suffer from unreality and ambiguity due to the imposed essence. Relying solely on a single image might not be sufficient to produce consistently successful facial reconstructions.

The researchers aimed to address the challenge of capturing facial details in 3D face reconstruction. To achieve this, they leveraged a 3D prior for facial details derived from a face scan corresponding to multi-view images. They collected two datasets: one containing raw 2D images and the other with corresponding 3D face scans. By fitting the raw 2D inputs to the 3D model, they calculated ground truth deformation and displacement maps, which were used in the pixel-to-pixel CGAN architecture for their reconstruction process.

In order to enhance the capture of facial details, the researchers introduced the de-retouching module. They recognized that a face image can be decomposed into three major parts: facial geometry, lighting, and face albedo. The face albedo captures the color of each image under natural lighting conditions but lacks the ability to represent skin texture like freckles, moles, and scars due to its low dimensionality. To overcome this limitation, they collected a high-quality dataset called the Face HD 100 dataset, containing 200 high-definition 3D meshes corresponding to multi-view images from 100 subjects. The dataset also included texture details essential for training the pixel-to-pixel CGAN model.

For the experiment, the researchers conducted two parts of implementation, using 2D in-the-wild images and 3D face scans with corresponding multi-view images. The training strategy involved using 600K images, with 260K being in-the-wild images for self-supervised training. Additionally, 150K images with corresponding ground truth details were utilized to train the pixel-to-pixel CGAN network.

The quantitative comparison was carried out using the Face Scans dataset and the RaFD dataset to evaluate the geometry accuracy of single-face reconstruction. They used metrics such as Chamfer Distance (CD) and Mean Boundary Error (MBE) to assess the quality of the reconstructed faces.

In conclusion, this research paper introduces a groundbreaking hierarchical representation network designed for accurate and detailed face reconstruction from in-the-wild images. The proposed network leverages a hierarchical representation learning approach for facial geometry, decomposing the face into low, mid, and high frequency details at different levels.

By incorporating a 3D prior for facial details, the model achieves exceptional reconstruction results in terms of both accuracy and visual fidelity. The use of ground truth deformation maps and ground truth displacement maps, obtained through the 3D prior details, enables the model to capture facial features more faithfully.

The introduction of the de-retouching module addresses the ambiguity between facial geometry and appearance. To construct this module, the researchers invested considerable effort and resources by hiring professional annotators to meticulously process high skin texture faces, enhancing the information available for face albedo.

Additionally, the researchers contribute to the field by introducing a high-quality 3D face dataset, named Face HD 100. This dataset encourages further research in the area of sparse view facial reconstructions.
